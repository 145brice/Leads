#!/usr/bin/env python3
"""
Incremental Scraper - Only pulls NEW leads, no duplicates
Tracks permit numbers and only adds unseen permits to database
"""

import requests
import json
import random
from datetime import datetime
from pathlib import Path

# Database path
DB_PATH = Path(__file__).parent / 'leads_db' / 'current_leads.json'

# ==================== DUPLICATE DETECTION ====================

def load_existing_leads():
    """Load current database and extract all permit numbers"""
    try:
        with open(DB_PATH, 'r') as f:
            db = json.load(f)
        
        # Extract all permit numbers from existing leads
        seen_permits = set()
        for state, counties in db.get('leads', {}).items():
            for county, leads in counties.items():
                for lead in leads:
                    permit_num = lead.get('permit_number', '')
                    if permit_num:
                        seen_permits.add(permit_num)
        
        print(f"üìä Loaded database: {len(seen_permits)} existing permits tracked")
        return db, seen_permits
    except FileNotFoundError:
        print("üìä No existing database found, creating new one")
        return {'last_updated': None, 'leads': {}}, set()

def is_duplicate(permit_number, seen_permits):
    """Check if permit number already exists"""
    return permit_number in seen_permits

def merge_new_leads(existing_db, new_leads_by_region, seen_permits):
    """Merge new leads into existing database, avoiding duplicates"""
    added_count = 0
    duplicate_count = 0
    
    for region_key, counties in new_leads_by_region.items():
        state, county = region_key.split('/')
        
        # Ensure structure exists
        if state not in existing_db['leads']:
            existing_db['leads'][state] = {}
        if county not in existing_db['leads'][state]:
            existing_db['leads'][state][county] = []
        
        for lead in counties:
            permit_num = lead.get('permit_number', '')
            
            if is_duplicate(permit_num, seen_permits):
                duplicate_count += 1
                print(f"   ‚è≠Ô∏è  Skipping duplicate: {permit_num}")
            else:
                # Add first_seen timestamp
                lead['first_seen'] = datetime.now().isoformat()
                existing_db['leads'][state][county].append(lead)
                seen_permits.add(permit_num)
                added_count += 1
                print(f"   ‚úÖ NEW: {permit_num} - {lead.get('address', 'Unknown')}")
    
    existing_db['last_updated'] = datetime.now().isoformat()
    
    return existing_db, added_count, duplicate_count

def save_database(db):
    """Save database to JSON file"""
    DB_PATH.parent.mkdir(exist_ok=True)
    with open(DB_PATH, 'w') as f:
        json.dump(db, f, indent=2)
    print(f"üíæ Database saved to {DB_PATH}")

# ==================== SCRAPERS (NO DUPLICATES) ====================

def scrape_nashville_davidson():
    """Nashville-Davidson County - REAL DATA from ArcGIS with PAGINATION (Last 30 days)"""
    permits = []
    try:
        print("üï∑Ô∏è  Scraping Nashville-Davidson County (ArcGIS API - Last 30 days)...")
        
        url = "https://maps.nashville.gov/arcgis/rest/services/Codes/BuildingPermits/MapServer/0/query"
        
        # Calculate 30 days ago timestamp (milliseconds since epoch)
        from datetime import timedelta
        thirty_days_ago = datetime.now() - timedelta(days=30)
        timestamp_ms = int(thirty_days_ago.timestamp() * 1000)
        
        # Pagination loop to overcome 20-record server limit
        offset = 0
        max_records = 5000  # Stop after 5000 records
        page_size = 20  # Server returns ~20 records per request
        
        while offset < max_records:
            params = {
                'where': f'DATE_ACCEPTED >= {timestamp_ms}',  # Last 30 days only
                'outFields': '*',
                'returnGeometry': 'false',
                'resultRecordCount': '1000',  # Request more, server limits to ~20
                'resultOffset': str(offset),  # Pagination offset
                'orderByFields': 'DATE_ACCEPTED DESC',
                'f': 'json'
            }
            
            response = requests.get(url, params=params, timeout=15)
            if response.status_code != 200:
                print(f"   ‚ö†Ô∏è  HTTP {response.status_code} at offset {offset}, stopping")
                break
            
            data = response.json()
            features = data.get('features', [])
            
            if not features:
                print(f"   ‚úÖ No more records at offset {offset}, stopping pagination")
                break
            
            # Process this batch
            for feature in features:
                attrs = feature.get('attributes', {})
                const_val = attrs.get('CONSTVAL', 0) or 0
                
                date_accepted = attrs.get('DATE_ACCEPTED')
                if date_accepted:
                    date_str = datetime.fromtimestamp(date_accepted / 1000).strftime('%Y-%m-%d')
                else:
                    date_str = 'N/A'
                
                permit = {
                    'permit_number': attrs.get('CASE_NUMBER', 'N/A'),
                    'address': attrs.get('LOCATION', 'N/A'),
                    'permit_type': attrs.get('CASE_TYPE_DESC', 'Building Permit'),
                    'estimated_value': float(const_val),
                    'work_description': (attrs.get('SCOPE', 'Construction project') or 'Construction project')[:200],
                    'score': 90,
                    'date': date_str,
                    'contractor': 'TBD',
                    'owner': 'Property Owner'
                }
                permits.append(permit)
            
            print(f"   üìÑ Page {offset//page_size + 1}: Got {len(features)} records (total: {len(permits)})")
            
            # Move to next page
            offset += len(features)
            
            # Safety check: if we got fewer than expected, probably at the end
            if len(features) < page_size:
                print(f"   ‚úÖ Last page returned {len(features)} records, stopping pagination")
                break
        
        print(f"   üîç Found {len(permits)} Nashville permits (with pagination!)")
    except Exception as e:
        print(f"   ‚ùå Nashville error: {e}")
    
    return permits

def scrape_dallas_county():
    """Dallas County - Sample data (API research needed)"""
    permits = []
    try:
        print("üï∑Ô∏è  Scraping Dallas County (Sample data)...")
        
        for i in range(5):
            permit = {
                'permit_number': f'DAL-2025-{6000+i+random.randint(1,999)}',
                'address': f'{i*300} Commerce St, Dallas, TX 75201',
                'permit_type': ['Commercial', 'High-Rise', 'Mixed-Use'][i % 3],
                'estimated_value': random.randint(200000, 800000),
                'work_description': 'Building construction permit',
                'score': 85,
                'date': datetime.now().strftime('%Y-%m-%d'),
                'contractor': 'TBD',
                'owner': 'Developer Corp'
            }
            permits.append(permit)
        
        print(f"   üîç Found {len(permits)} Dallas sample permits")
    except Exception as e:
        print(f"   ‚ùå Dallas error: {e}")
    
    return permits

def scrape_austin_travis():
    """Austin-Travis County - REAL DATA from Socrata API (Last 30 days)"""
    permits = []
    try:
        print("üï∑Ô∏è  Scraping Austin-Travis County (Socrata API - Last 30 days)...")
        
        url = "https://data.austintexas.gov/resource/3syk-w9eu.json"
        
        # Calculate 30 days ago in ISO format
        from datetime import timedelta
        thirty_days_ago = datetime.now() - timedelta(days=30)
        date_filter = thirty_days_ago.strftime('%Y-%m-%d')
        
        params = {
            '$limit': '5000',
            '$order': 'applieddate DESC',
            '$where': f"permit_class_mapped='Residential' AND applieddate >= '{date_filter}'"
        }
        
        response = requests.get(url, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()
        
        for record in data:
            value = 0
            if record.get('total_job_valuation'):
                try:
                    value = int(float(record['total_job_valuation']))
                except:
                    pass
            
            permit = {
                'permit_number': record.get('permit_number', 'Unknown'),
                'address': record.get('permit_location', 'Unknown'),
                'permit_type': record.get('permit_type_desc', 'Building Permit'),
                'estimated_value': value if value > 0 else 0,
                'work_description': record.get('description', 'No description')[:200],
                'score': 88,
                'date': record.get('applieddate', '').split('T')[0] if record.get('applieddate') else None,
                'contractor': 'TBD',
                'owner': 'Property Owner'
            }
            permits.append(permit)
        
        print(f"   üîç Found {len(permits)} Austin permits")
    except Exception as e:
        print(f"   ‚ùå Austin error: {e}")
    
    return permits

def scrape_san_antonio_bexar():
    """San Antonio-Bexar County - REAL DATA from OpenGov CSV (Last 30 days)"""
    permits = []
    try:
        print("üï∑Ô∏è  Scraping San Antonio-Bexar County (OpenGov CSV - Last 30 days)...")
        
        from datetime import timedelta
        thirty_days_ago = datetime.now() - timedelta(days=30)
        
        csv_url = 'https://data.sanantonio.gov/dataset/05012dcb-ba1b-4ade-b5f3-7403bc7f52eb/resource/fbb7202e-c6c1-475b-849e-c5c2cfb65833/download/accelasubmitpermitsextract.csv'
        
        response = requests.get(csv_url, timeout=30)
        response.raise_for_status()
        
        import csv
        from io import StringIO
        
        csv_data = StringIO(response.text)
        reader = csv.DictReader(csv_data)
        
        count = 0
        for row in reader:
            permit_type = row.get('PERMIT TYPE', '')
            if not any(keyword in permit_type.lower() for keyword in ['building', 'commercial', 'residential', 'mep', 'trade', 'repair']):
                continue
            
            # Check date is within last 30 days
            date_issued = row.get('DATE ISSUED', '')
            if date_issued:
                try:
                    from datetime import datetime as dt
                    permit_date = dt.strptime(date_issued.split()[0], '%m/%d/%Y')
                    if permit_date < thirty_days_ago:
                        continue
                except:
                    pass  # If date parsing fails, include the permit
            
            permit = {
                'permit_number': row.get('PERMIT #', ''),
                'address': row.get('ADDRESS', ''),
                'permit_type': permit_type,
                'estimated_value': int(float(row.get('DECLARED VALUATION', 0) or 0)),
                'work_description': row.get('WORK TYPE', '')[:200],
                'score': 86,
                'date': date_issued,
                'contractor': row.get('PRIMARY CONTACT', 'TBD'),
                'owner': row.get('PRIMARY CONTACT', 'TBD')
            }
            permits.append(permit)
            count += 1
            
            if count >= 5000:  # Increased limit since we're filtering by date
                break
        
        print(f"   üîç Found {len(permits)} San Antonio permits")
    except Exception as e:
        print(f"   ‚ùå San Antonio error: {e}")
    
    return permits

# ==================== MAIN SCRAPING FUNCTION ====================

def scrape_all_regions_incremental():
    """Scrape all regions and only add new leads"""
    print("\n" + "="*70)
    print("üåê INCREMENTAL SCRAPING SESSION - NO DUPLICATES")
    print("="*70)
    
    # Load existing database
    existing_db, seen_permits = load_existing_leads()
    
    # Scrape each region
    new_leads_by_region = {}
    
    # Nashville-Davidson (Tennessee)
    nashville_leads = scrape_nashville_davidson()
    if nashville_leads:
        new_leads_by_region['tennessee/nashville'] = nashville_leads
    
    # Dallas (Texas)
    dallas_leads = scrape_dallas_county()
    if dallas_leads:
        new_leads_by_region['texas/dallas'] = dallas_leads
    
    # Austin-Travis (Texas)
    austin_leads = scrape_austin_travis()
    if austin_leads:
        new_leads_by_region['texas/travis'] = austin_leads
    
    # San Antonio-Bexar (Texas)
    san_antonio_leads = scrape_san_antonio_bexar()
    if san_antonio_leads:
        new_leads_by_region['texas/bexar'] = san_antonio_leads
    
    # Merge new leads (avoiding duplicates)
    print("\n" + "="*70)
    print("üîç CHECKING FOR DUPLICATES")
    print("="*70)
    
    updated_db, added_count, duplicate_count = merge_new_leads(
        existing_db, 
        new_leads_by_region, 
        seen_permits
    )
    
    # Save updated database
    save_database(updated_db)
    
    # Summary
    print("\n" + "="*70)
    print("üìä SCRAPING SUMMARY")
    print("="*70)
    print(f"‚úÖ New leads added: {added_count}")
    print(f"‚è≠Ô∏è  Duplicates skipped: {duplicate_count}")
    print(f"üì¶ Total leads in database: {len(seen_permits) + added_count}")
    print(f"üïí Last updated: {updated_db['last_updated']}")
    print("="*70 + "\n")
    
    return {
        'added': added_count,
        'duplicates': duplicate_count,
        'total': len(seen_permits) + added_count
    }

if __name__ == '__main__':
    scrape_all_regions_incremental()
