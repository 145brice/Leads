# ğŸš€ AUTO-SUBSCRIPTION SYSTEM - Complete Setup Guide

**Date:** November 19, 2025  
**System:** Contractor Leads Auto-Feed Platform

---

## ğŸ“‹ **What We Built**

### Core Features:
1. **Stripe Checkout** - $20/month per city, 7-day free trial
2. **Auto-Scraper Cron** - Runs 4x daily (5:30 AM, 9:30 AM, 1:30 PM, 5:30 PM)
3. **Duplicate Detection** - SHA-256 hash-based deduplication (30-day rolling window)
4. **Email Delivery** - Daily CSV dumps + RSS feeds for CRM integration
5. **Archive Vault** - 70k San Antonio original dump preserved forever

---

## ğŸ—‚ï¸ **File Structure**

```
contractor-leads-saas/
â”œâ”€â”€ subscription_manager.py      â† Stripe payments, subscriber database
â”œâ”€â”€ subscription_app.py          â† Flask web app with checkout UI
â”œâ”€â”€ auto_scraper_cron.py         â† 4x daily scraping daemon
â”œâ”€â”€ email_service.py             â† Email with CSV attachments
â”œâ”€â”€ multi_region_scraper.py      â† Core scraping engine
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ subscription_index.html  â† Beautiful city selection UI
â”œâ”€â”€ archive_vault/               â† 70k original dumps (NEVER DELETE)
â”‚   â””â”€â”€ San Antonio-Bexar_archive.csv
â”œâ”€â”€ fresh_feeds/                 â† Daily new permits only
â”‚   â””â”€â”€ user_12345_Nashville_20251119.csv
â”œâ”€â”€ rss_feeds/                   â† RSS XML for CRM integration
â”‚   â””â”€â”€ Nashville-Davidson.xml
â””â”€â”€ subscriptions.db             â† SQLite database
```

---

## ğŸ¯ **System Flow**

### 1. User Subscribes ($20/month, 7-day trial)
```
User visits â†’ http://localhost:5002
â†“
Clicks "Buy" on Nashville
â†“
Stripe Checkout â†’ Payment processed
â†“
Database: subscriptions table updated
â†“
Auto-scraper: Adds Nashville to daily rotation
```

### 2. Auto-Scraper Runs (4x Daily)
```
Cron triggers at 5:30 AM
â†“
Scrapes all subscribed cities (Nashville, Austin, San Antonio, etc.)
â†“
Filters NEW permits (SHA-256 hash check)
â†“
Saves to fresh_feeds/user_id_city_timestamp.csv
â†“
Emails subscriber with CSV attached
â†“
Updates RSS feed for CRM integration
```

### 3. Duplicate Prevention
```
Permit scraped â†’ Generate SHA-256 hash (address + type + date)
â†“
Check seen_permits table
â†“
IF duplicate â†’ Skip
IF new â†’ Mark as seen + deliver to subscriber
â†“
Cleanup: Delete hashes older than 30 days
```

---

## ğŸ› ï¸ **Setup Instructions**

### Step 1: Install Dependencies
```bash
cd /Users/briceleasure/Desktop/contractor-leads-saas

# Install Python packages
pip3 install stripe sendgrid schedule

# Or add to requirements.txt:
echo "stripe" >> requirements.txt
echo "sendgrid" >> requirements.txt
echo "schedule" >> requirements.txt
pip3 install -r requirements.txt
```

### Step 2: Configure Environment Variables
```bash
# Create .env file
cat >> .env << 'EOF'
# Stripe Configuration
STRIPE_PUBLISHABLE_KEY=pk_live_...
STRIPE_SECRET_KEY=sk_live_...
STRIPE_WEBHOOK_SECRET=whsec_...

# SendGrid Email
SENDGRID_API_KEY=SG....
SENDGRID_FROM_EMAIL=leads@contractorleads.com

# Flask
FLASK_SECRET_KEY=your-secret-key-here
EOF
```

### Step 3: Initialize Database
```bash
python3 subscription_manager.py
```

**Output:**
```
======================================================================
ğŸ—ï¸  SUBSCRIPTION MANAGER SETUP
======================================================================
âœ… Database initialized

ğŸ“‹ Configured Cities:
   â€¢ Nashville-Davidson          â†’ $20.00/month (7-day trial)
   â€¢ Austin-Travis               â†’ $20.00/month (7-day trial)
   â€¢ Chattanooga-Hamilton        â†’ $20.00/month (7-day trial)
   â€¢ San Antonio-Bexar           â†’ $20.00/month (7-day trial)

ğŸ“‚ Directories:
   â€¢ Archive Vault: /Users/briceleasure/Desktop/contractor-leads-saas/archive_vault
   â€¢ Fresh Feeds:   /Users/briceleasure/Desktop/contractor-leads-saas/fresh_feeds
   â€¢ RSS Feeds:     /Users/briceleasure/Desktop/contractor-leads-saas/rss_feeds
   â€¢ Database:      /Users/briceleasure/Desktop/contractor-leads-saas/subscriptions.db

âœ… Setup complete!
```

### Step 4: Start Subscription Web App
```bash
python3 subscription_app.py
```

**Visit:** http://localhost:5002

You'll see:
- Beautiful UI with all cities
- $20/month pricing with 7-day trial
- "Buy" buttons for each city
- Real-time status (LIVE vs Coming Soon)

### Step 5: Test Auto-Scraper (Once)
```bash
python3 auto_scraper_cron.py --once
```

**Output:**
```
======================================================================
ğŸ¤– AUTO-SCRAPER RUNNING - 2025-11-19 13:50:00
======================================================================

ğŸ“ Cities to scrape: 3
   â€¢ Nashville-Davidson: 2 subscribers
   â€¢ Austin-Travis: 1 subscriber
   â€¢ San Antonio-Bexar: 1 subscriber

ğŸ•·ï¸  Scraping Nashville-Davidson...
   âœ… Scraped 80 total permits
   ğŸ†• Found 15 NEW permits!
   ğŸ“§ Sent to user1@email.com
   ğŸ“§ Sent to user2@email.com

ğŸ•·ï¸  Scraping Austin-Travis...
   âœ… Scraped 100 total permits
   ğŸ†• Found 22 NEW permits!
   ğŸ“§ Sent to user3@email.com

ğŸ•·ï¸  Scraping San Antonio-Bexar...
   âœ… Scraped 120 total permits
   ğŸ†• Found 35 NEW permits!
   ğŸ“§ Sent to user4@email.com

ğŸ§¹ Cleaned up 147 old permit records

======================================================================
âœ… Scraping complete!
======================================================================
```

### Step 6: Run Auto-Scraper as Daemon (4x Daily)
```bash
# Run in background
nohup python3 auto_scraper_cron.py --daemon > scraper.log 2>&1 &

# Or use systemd service (production):
sudo systemctl start contractor-scraper
```

---

## ğŸ“Š **Database Schema**

### `subscriptions` table:
```sql
CREATE TABLE subscriptions (
    id INTEGER PRIMARY KEY,
    user_id TEXT NOT NULL,
    email TEXT NOT NULL,
    city TEXT NOT NULL,
    stripe_customer_id TEXT,
    stripe_subscription_id TEXT,
    status TEXT DEFAULT 'active',
    created_at TIMESTAMP,
    trial_end TIMESTAMP,
    UNIQUE(user_id, city)
);
```

### `seen_permits` table:
```sql
CREATE TABLE seen_permits (
    id INTEGER PRIMARY KEY,
    city TEXT NOT NULL,
    permit_hash TEXT NOT NULL,
    permit_number TEXT,
    address TEXT,
    scraped_at TIMESTAMP,
    UNIQUE(city, permit_hash)
);
```

---

## ğŸ¨ **UI Features**

### Subscription Page (http://localhost:5002)
- **Gradient background** - Purple/blue professional look
- **City cards** - Nashville, Austin, San Antonio, Chattanooga
- **Status badges** - "âœ“ LIVE" vs "Coming Soon"
- **Stats** - Avg permits/day, update frequency
- **Buy buttons** - "$20/month Â· 7-Day Free Trial Â· Cancel Anytime"
- **Feature list** - Real-time data, CSV dumps, RSS feeds, no duplicates
- **Info banner** - How it works (5 steps)

### After Purchase:
```
ğŸ‰ Subscription Activated!
Your auto-feed is now running. Expect your first permits at 5:30 AM tomorrow.
```

---

## ğŸ“§ **Email Template**

Subscribers receive:

```
Subject: ğŸ—ï¸ 15 New Building Permits in Nashville-Davidson

Body:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ—ï¸ Fresh Building Permits       â”‚
â”‚          Nashville-Davidson          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

You have 15 new leads!

Scraped: November 19, 2025 at 5:31 AM
City: Nashville-Davidson
New Permits: 15

Your fresh permits are attached as a CSV file. 
These are brand new leads - no duplicates, just 
opportunities scraped in the last few hours.

ğŸ’¡ Pro Tip: The earliest contractors usually win 
the bid. Call these leads within the hour for 
best results.

[CSV Attachment: user_12345_Nashville_20251119_053100.csv]

Next scrape: Every 4 hours (5:30 AM, 9:30 AM, 1:30 PM, 5:30 PM)
```

---

## ğŸ”„ **Cron Schedule**

Auto-scraper runs 4x daily:
- **5:30 AM** - Morning scrape (largest batch)
- **9:30 AM** - Mid-morning scrape
- **1:30 PM** - Afternoon scrape
- **5:30 PM** - Evening scrape

Email sent immediately after each scrape (only if new permits found).

---

## ğŸ’¾ **Archive Management**

### Original 70k San Antonio Dump:
```
archive_vault/San Antonio-Bexar_archive.csv
```

**Rules:**
- âœ… **NEVER DELETE** - This is the master archive
- âœ… **Preserved forever** - Historical record
- âœ… **Separate from daily feeds** - Not sent to subscribers
- âœ… **Optional download** - Available on request

### Daily Fresh Feeds:
```
fresh_feeds/user_12345_Nashville_20251119_053100.csv
```

**Rules:**
- âœ… **Only NEW permits** - Filtered against seen_permits table
- âœ… **No duplicates** - SHA-256 hash deduplication
- âœ… **Auto-emailed** - Sent immediately after scrape
- âœ… **30-day retention** - Old files auto-deleted

---

## ğŸš¨ **Important Notes**

### 1. **No Duplicates Strategy**
- **Hash-based:** SHA-256(address + permit_type + issue_date)
- **Rolling window:** 30-day seen_permits table
- **Auto-cleanup:** Old hashes deleted daily
- **Result:** Subscribers NEVER see same permit twice

### 2. **Archive Preservation**
- **San Antonio archive:** 72,986 total permits preserved
- **Never touched by scraper** - Separate directory
- **Building permits only:** 22,519 filtered (no garage sales)
- **Available for:** Historical analysis, bulk downloads, research

### 3. **Payment Flow**
1. User clicks "Buy" â†’ Stripe Checkout
2. Stripe processes payment â†’ Webhook fires
3. Database updated â†’ Subscription activated
4. Cron adds city to rotation â†’ Auto-scraping begins
5. First email sent at 5:30 AM next day

### 4. **Cancellation**
- User cancels in Stripe dashboard
- Webhook fires â†’ Database updated (status='cancelled')
- Cron stops scraping for that user
- No more emails sent
- Archive preserved (never deleted)

---

## ğŸ§ª **Testing Commands**

### Test Single City Scrape:
```bash
python3 -c "
from subscription_manager import filter_new_permits, save_fresh_dump
from multi_region_scraper import scrape_all_regions

permits = scrape_all_regions(['Nashville'])
new_permits = filter_new_permits('Nashville-Davidson', permits)
csv_file = save_fresh_dump('Nashville-Davidson', 'test_user', new_permits)
print(f'Saved {len(new_permits)} new permits to {csv_file}')
"
```

### Test Email Delivery:
```bash
python3 -c "
from email_service import send_permit_email
send_permit_email(
    to_email='your@email.com',
    city='Nashville-Davidson',
    permit_count=15,
    csv_file='fresh_feeds/test.csv'
)
"
```

### Check Database:
```bash
sqlite3 subscriptions.db "SELECT * FROM subscriptions;"
sqlite3 subscriptions.db "SELECT COUNT(*) FROM seen_permits;"
```

---

## ğŸš€ **Production Deployment**

### 1. Set up Stripe Products:
```bash
# Create products in Stripe dashboard:
- Nashville Building Permits Feed ($20/month, 7-day trial)
- Austin Building Permits Feed ($20/month, 7-day trial)
- San Antonio Building Permits Feed ($20/month, 7-day trial)
- Chattanooga Building Permits Feed ($20/month, 7-day trial)
```

### 2. Configure Webhook:
```
Stripe Dashboard â†’ Webhooks â†’ Add endpoint
URL: https://yoursite.com/webhook
Events: checkout.session.completed, customer.subscription.deleted
```

### 3. Deploy Flask App:
```bash
gunicorn -w 4 -b 0.0.0.0:5002 subscription_app:app
```

### 4. Deploy Auto-Scraper:
```bash
# Systemd service
sudo cp contractor-scraper.service /etc/systemd/system/
sudo systemctl enable contractor-scraper
sudo systemctl start contractor-scraper
```

---

## ğŸ“ˆ **Metrics to Track**

- **Active subscribers:** SELECT COUNT(*) FROM subscriptions WHERE status='active'
- **Total revenue:** $20 Ã— active_subscribers
- **Permits scraped:** SELECT COUNT(*) FROM seen_permits
- **Email success rate:** Check SendGrid dashboard
- **Churn rate:** Cancelled subscriptions / total subscriptions

---

## âœ… **Success Checklist**

- [ ] Stripe keys configured
- [ ] SendGrid API key set
- [ ] Database initialized
- [ ] Archive vault created (San Antonio 70k dump)
- [ ] Subscription app running (port 5002)
- [ ] Auto-scraper daemon running (4x daily)
- [ ] Test subscription completed successfully
- [ ] Test email received with CSV attachment
- [ ] Duplicate detection working (no repeat permits)
- [ ] Webhooks configured in Stripe
- [ ] Production deployment ready

---

**ğŸ‰ System Complete! Contractors get fresh leads. You get recurring revenue. Magic. ğŸš€**
